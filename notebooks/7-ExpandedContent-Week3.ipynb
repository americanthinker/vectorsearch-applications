{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b11bcbb-3763-4c0e-bd10-aac9aef50e09",
   "metadata": {},
   "source": [
    "#### Week 3: Building Advanced RAG Applications.  Authored by Chris Sanchez.\n",
    "\n",
    "# Week 3 - Notebook 7 --> Context Enrichment\n",
    "\n",
    "# Overview\n",
    "---\n",
    "This notebook will walk you through the process of creating an `expanded_content` field that you can add to an existing dataset, which can then be indexed onto your Weaviate cluster. \n",
    "- No need to create a new dataset, simpy use a prexisting dataset (i.e. `huberman_minilm_256.parquet`)\n",
    "- Group dataset episodes together by `video_id`.  Performing this step will ensure that all before and after text chunks are all from the same episode and there is no \"bleed-over\" into another episode.\n",
    "- Loop over each set of episode chunks and join pre-, current, and post- chunks together as a single string.  The window size can be set as a parameter.\n",
    "- Join each chunk to the original dataset as an additional `expanded_content` field.\n",
    "- Either index the new dataset on a new collection, or update an existing collection.  The properties file already includes an `expanded_content` property. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ebd912-6b31-475e-86d9-4fac57cfcdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d6c6cc-86a5-46ac-ad15-ddfe21331a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97a9ba7a-2fde-4625-a963-c37335270d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.database.properties_template import properties\n",
    "from src.database.database_utils import get_weaviate_client\n",
    "from src.preprocessor.preprocessing import FileIO\n",
    "# from llama_index.text_splitter import SentenceSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014acbc2-478d-404d-918f-639ec5fd2b76",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "No need to create a new dataset, simply use the data that you already have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a649a419-30a9-48bc-b13d-df872af711c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/vsa/lib/python3.10/site-packages/pandas/core/frame.py:717: DeprecationWarning: Passing a BlockManager to DataFrame is deprecated and will raise in a future version. Use public APIs instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (23905, 13)\n",
      "Memory Usage: 2.37+ MB\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/huberman_minilm-256.parquet'\n",
    "data = FileIO.load_parquet(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266d1b3c-b6ca-4c80-9006-838b10890788",
   "metadata": {},
   "source": [
    "### Create Expanded Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "039f0afd-690f-465c-b34f-e236bdf29ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def groupby_episode(data: list[dict], key_field: str='video_id') -> list[list[dict]]:\n",
    "    '''\n",
    "    Separates entire Impact Theory corpus into individual \n",
    "    lists of discrete episodes.\n",
    "    '''\n",
    "    episodes = []\n",
    "    for _, group in groupby(data, lambda x: x[key_field]):\n",
    "        episode = [chunk for chunk in group]\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "850f8a51-36c0-4266-98c1-e5720de44dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_expanded_content(data: list[dict]=None, \n",
    "                            chunk_list: list[list[str]]=None, \n",
    "                            window_size: int=1,\n",
    "                            num_episodes: int=193,\n",
    "                            key_field: str='video_id'\n",
    "                            ) -> list[list[str]]:\n",
    "    '''\n",
    "    Creates expanded content from original chunks of text, for use with \n",
    "    expanded content retrieval.  Takes in raw data in dict format or \n",
    "    accepts a list of chunked episodes already grouped. \n",
    "    \n",
    "    Window size sets the number of chunks before and after the original chunk.  \n",
    "    For example a window_size of 2 will return five joined chunks.  2 chunks \n",
    "    before original chunk, the original, and 2 chunks after the original.  \n",
    "    \n",
    "    Expanded content is grouped by podcast episode, and chunks are assumed \n",
    "    to be kept in order by which they will be joined as metadata in follow-on \n",
    "    processing.\n",
    "    '''\n",
    "    if not data and not chunk_list:\n",
    "        raise ValueError(\"Either data or a chunk_list must be passed as an arg\")\n",
    "        \n",
    "    if data:\n",
    "        # groupby data into episodes using video_id key\n",
    "        episodes = groupby_episode(data, key_field)\n",
    "        assert len(episodes) == num_episodes, f'Number of grouped episodes does not equal num_episodes ({len(episodes)} != {num_episodes})'\n",
    "\n",
    "        # extract content field and ensure episodes maintain their grouping\n",
    "        chunk_list = [[d['content'] for d in alist] for alist in episodes]\n",
    "        \n",
    "    expanded_contents = []\n",
    "    for episode in tqdm(chunk_list):\n",
    "        episode_container = []\n",
    "        for i, chunk in enumerate(episode):\n",
    "            start = max(0, i-window_size)\n",
    "            end = i+window_size+1\n",
    "            expanded_content = ' '.join(episode[start:end])\n",
    "            episode_container.append(expanded_content)\n",
    "        expanded_contents.append(episode_container)\n",
    "    return expanded_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd792d-e257-455a-998f-e4ae9262891a",
   "metadata": {},
   "source": [
    "# Assignment 3.1 - \n",
    "***\n",
    "#### *Create Expanded Content chunks and join them to existing data*\n",
    "\n",
    "#### INSTRUCTIONS\n",
    "1. Execute the `create_expanded_content` function.  Depending on your chunk size is likely best to use the default window size of 1.  Meaning, 1 chunk of text will be added before and after the original text chunk, for a total of three chunks for each `expanded_content` field.\n",
    "2. Assuming you are going to join the data back to the original dataset from which it came, you'll need to flatten out the list of episode into a single list of text chunks.\n",
    "3. Write a function that combined your original dataset with the new expanded content by updating the dataset with an `expanded_content` key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "022f2d97-56f6-4596-bb53-222c3bb020b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# START YOUR CODE HERE #\n",
    "########################\n",
    "\n",
    "expanded_content = create_expanded_content(None)\n",
    "flattened_content = None\n",
    "\n",
    "# azimuth check to ensure you're heading in the right direction\n",
    "flat_length = len(flattened_content)\n",
    "data_legnth = len(data)\n",
    "assert flat_length == data_length, 'Mismatch in lengths. Double check how you flattened your expanded_content'\n",
    "\n",
    "def join_expanded_content(data: list[dict],\n",
    "                          flattened_content: list[list[str]]\n",
    "                          ) -> list[dict]:\n",
    "    '''\n",
    "    Updates data with an expanded_content key.\n",
    "    '''\n",
    "\n",
    "    \n",
    "########################\n",
    "# END YOUR CODE HERE #\n",
    "########################\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = join_expanded_content(None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1501aadc-1843-4e23-b706-2a54c19677f4",
   "metadata": {},
   "source": [
    "#### After executing the above function, run the following cell as a post-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cc86127-1cf9-4f4b-bf59-3b83ef266cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    assert d.get('expanded_content', -1) != -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e417972-3a16-4236-b3af-4056c62eefc3",
   "metadata": {},
   "source": [
    "### Index the Data\n",
    "---\n",
    "You have two options here:\n",
    "1. Easy way: Simply index the data on a new Collection.\n",
    "2. Hard way: Read all existing uuids on current Collection and update each object by linking the doc_ids.\n",
    "\n",
    "As mentioned earlier, the expanded_content property is already part of the index configuration of properties.  See the last property entry after printing the `properties` variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93deae35-08b2-4eec-b8da-7e6743f83db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "\n",
    "# print(properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d9822c-5d7c-4f57-8719-902f65123c65",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---\n",
    "After you've indexed the data you will now have a way to retrieve content on a fine-grained level, and provide your LLM Reader with an expanded context.  You will be able to see this in action when you add `expanded_content` as a `return_property` in your Streamlit UI. ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea8276-f8b1-4f8f-b76d-25d66d3d09ae",
   "metadata": {},
   "source": [
    "## OPTIONAL: Update an existing Collection\n",
    "---\n",
    "For those interested in doing things the hard way here is some starter code.  No guarantee that this code will work as written, but it gives you the idea of how you would accomplish this task; or just create a new Collection... ðŸ˜€:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac81c228-d7d9-4db7-8cf7-5ec4fa326594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get collection object\n",
    "client = get_weaviate_client()\n",
    "collection = client._client.collections.get('Huberman_minilm_256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fde8c03-0d00-43c5-a837-80b2597c8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step will take a few minutes to read every object id on the Weaviate cluster\n",
    "doc_id_cache = {item.properties['doc_id']:item.uuid for item in tqdm(collection.iterator())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c13cc3-c9bd-4a59-bc1c-a2c79dc93b03",
   "metadata": {},
   "source": [
    "`doc_id_cache` example:\n",
    "```\n",
    "{'-OBCwiPPfEU_8': _WeaviateUUIDInt('018455e9-47ab-41cc-b592-c431fd8df75f'),\n",
    " '-OBCwiPPfEU_4': _WeaviateUUIDInt('03a18709-0334-4f18-9375-4a8a3f162cfb'),\n",
    " '-OBCwiPPfEU_0': _WeaviateUUIDInt('0da24442-3263-46d7-91af-ef2a34d27a9c'),\n",
    " '-OBCwiPPfEU_1': _WeaviateUUIDInt('219354b1-dd2e-46c0-94cb-cdd51f915175'),\n",
    " '-OBCwiPPfEU_6': _WeaviateUUIDInt('27f967f2-3e9c-453d-8d21-378e4e15ffac'),\n",
    " '-OBCwiPPfEU_7': _WeaviateUUIDInt('332a363f-afcf-4fb7-9370-bbded23b8803'),\n",
    " '-OBCwiPPfEU_3': _WeaviateUUIDInt('59c539ac-8f95-4b13-bb6c-1fb323d7e64a'),\n",
    " '-OBCwiPPfEU_2': _WeaviateUUIDInt('746401bd-98c2-4494-ba08-d1b21d9abfc5'),\n",
    " '-OBCwiPPfEU_9': _WeaviateUUIDInt('77a3a7ae-8d77-4ae5-a0cd-705fb4286198'),\n",
    " '-OBCwiPPfEU_5': _WeaviateUUIDInt('803d2756-e3bd-44ef-88e6-43bc700be480')}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539cfe19-6b67-4e5f-8ef2-e5d1de1b43e8",
   "metadata": {},
   "source": [
    "##### Finally you'll want to loop through your dataset, grab the doc_id value and expanded_content value and then update each object on the Weaviate cluster by using the uuid as found on the doc_id_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e252b3e-ddb7-440e-adf5-1121f52d1300",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    doc_id = d['doc_id']\n",
    "    expanded_content = d['expanded_content']\n",
    "    uuid = doc_id_cache[doc_id]\n",
    "    collection.data.update(uuid=uuid, properties={'expanded_content': expanded_content}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vsa",
   "language": "python",
   "name": "vsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
