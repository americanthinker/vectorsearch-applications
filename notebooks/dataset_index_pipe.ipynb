{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "208b6f8b-09bb-4e2c-bf8b-a5410ceffebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#load from local .env file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "#standard libraries\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import List\n",
    "from math import ceil\n",
    "\n",
    "#external libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rich import print\n",
    "from torch import cuda\n",
    "from tqdm import tqdm\n",
    "import tiktoken # bad ass tokenizer library for use with OpenAI LLMs \n",
    "from llama_index.legacy.text_splitter import SentenceSplitter #one of the best on the market\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#external files\n",
    "from src.preprocessor.preprocessing import FileIO\n",
    "from src.database.weaviate_interface_v4 import WeaviateWCS, WeaviateIndexer\n",
    "from src.database.properties_template import properties\n",
    "from src.pipelines.pipeline import (chunk_data, create_vectors, join_docs, \n",
    "                                    create_dataset, groupby_episode, create_parent_chunks,\n",
    "                                    convert_raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2707da2-956c-4400-b055-cacd674eed00",
   "metadata": {},
   "source": [
    "### Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95de892-f23c-4100-9e14-554eef5921cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128\n",
    "\n",
    "#tokenizer\n",
    "encoding = tiktoken.get_encoding(encoding_name='cl100k_base')\n",
    "#text_splitter\n",
    "splitter = SentenceSplitter(chunk_overlap=0, chunk_size=chunk_size, tokenizer=encoding.encode)\n",
    "#model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cuda:0')\n",
    "#corpus\n",
    "data = FileIO().load_json('../data/huberman_labs.json')\n",
    "# data = convert_raw_data(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274457c0-b4a6-49e0-8743-ef02da5264ee",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4bb1e48-ad59-4a96-b520-963715ae2692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating dataset using chunk_size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating dataset using chunk_size: \u001b[1;36m128\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4df010d54048079da24c4f1858a07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CHUNKING:   0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[0;32m~/vectorsearch-applications/notebooks/../src/pipelines/pipeline.py:91\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m(corpus, embedding_model, text_splitter, file_outpath_prefix, unique_id_field, content_field, embedding_field, overwrite_existing, device)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreating dataset using chunk_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     89\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m---> 91\u001b[0m content_splits \u001b[38;5;241m=\u001b[39m \u001b[43mchunk_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_field\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m text_vector_tuples \u001b[38;5;241m=\u001b[39m create_vectors(content_splits, embedding_model, device)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/vectorsearch-applications/notebooks/../src/pipelines/pipeline.py:18\u001b[0m, in \u001b[0;36mchunk_data\u001b[0;34m(data, text_splitter, content_field)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_data\u001b[39m(data: List[\u001b[38;5;28mdict\u001b[39m], text_splitter: SentenceSplitter, content_field\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text_splitter\u001b[38;5;241m.\u001b[39msplit_text(d[content_field]) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m tqdm(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCHUNKING\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "File \u001b[0;32m~/vectorsearch-applications/notebooks/../src/pipelines/pipeline.py:18\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_data\u001b[39m(data: List[\u001b[38;5;28mdict\u001b[39m], text_splitter: SentenceSplitter, content_field\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcontent_field\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m tqdm(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCHUNKING\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vsa/lib/python3.10/site-packages/llama_index/legacy/node_parser/text/sentence.py:171\u001b[0m, in \u001b[0;36mSentenceSplitter.split_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vsa/lib/python3.10/site-packages/llama_index/legacy/node_parser/text/sentence.py:185\u001b[0m, in \u001b[0;36mSentenceSplitter._split_text\u001b[0;34m(self, text, chunk_size)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text]\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    183\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mCHUNKING, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: [text]}\n\u001b[1;32m    184\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 185\u001b[0m     splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge(splits, chunk_size)\n\u001b[1;32m    188\u001b[0m     event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: chunks})\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vsa/lib/python3.10/site-packages/llama_index/legacy/node_parser/text/sentence.py:202\u001b[0m, in \u001b[0;36mSentenceSplitter._split\u001b[0;34m(self, text, chunk_size)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_split\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, chunk_size: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_Split]:\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Break text into splits that are smaller than chunk size.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    The order of splitting is:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     token_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_token_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_size(text) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m chunk_size:\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_Split(text, is_sentence\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, token_size\u001b[38;5;241m=\u001b[39mtoken_size)]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vsa/lib/python3.10/site-packages/llama_index/legacy/node_parser/text/sentence.py:303\u001b[0m, in \u001b[0;36mSentenceSplitter._token_size\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_token_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vsa/lib/python3.10/site-packages/tiktoken/core.py:124\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[0;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[1;32m    121\u001b[0m     allowed_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(allowed_special)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core_bpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_special\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# BPE operates on bytes, but the regex operates on unicode. If we pass a str that is\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# invalid UTF-8 to Rust, it will rightfully complain. Here we do a quick and dirty\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# string, but given that this is input we want to support, maybe that's okay.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# Also we use errors=\"replace\" to handle weird things like lone surrogates.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outpath = '../data/huberman_minilm'\n",
    "docs = create_dataset(data, model, splitter, file_outpath_prefix=outpath, overwrite_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a48dbfd-7975-4a8e-ad67-620ebbf71e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coerce_to_int(data: list[dict], \n",
    "                  fields: list[str]=['length_seconds', 'view_count']\n",
    "                 ) -> None:\n",
    "    for d in data:\n",
    "        for field in fields:\n",
    "            d[field] = int(d[field])\n",
    "    for d in data:\n",
    "        for field in fields:\n",
    "            assert isinstance(d[field], int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fdbd229-2c89-4559-86cd-f36abde9b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coerce_to_int(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa8644d-7926-4b5d-b896-74700dc1037f",
   "metadata": {},
   "source": [
    "### Create Expanded Content property "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11cd471-5aa0-4a87-961b-3eb8ce5ad41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped = groupby_episode(docs, key_field='video_id')\n",
    "# pchunks = create_parent_chunks(grouped, window_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ad54a55-e1a4-409a-9de9-ce4fa990734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, chunk in enumerate(pchunks):\n",
    "#     doc_id = list(chunk.keys())[0]\n",
    "#     assert doc_id == docs[i]['doc_id'], f'failed at line {i}\\t{k}'\n",
    "#     docs[i]['expanded_content'] = chunk[doc_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc86d450-7261-4b57-8814-14258f8d5a7f",
   "metadata": {},
   "source": [
    "### Create Weaviate Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d7c7618-f810-49b2-a00a-aa5573723a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3;92mTrue\u001b[0m \u001b[3;92mTrue\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#read env vars from local .env file\n",
    "api_key = os.environ['WEAVIATE_API_KEY']\n",
    "url = os.environ['WEAVIATE_ENDPOINT']\n",
    "\n",
    "#instantiate client\n",
    "client = WeaviateWCS(url, api_key=api_key)\n",
    "\n",
    "#check if WCS instance is live and ready\n",
    "print(client._client.is_live(), client._client.is_ready())\n",
    "\n",
    "indexer = WeaviateIndexer(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9ac06-1d2c-45dc-9c92-ba35e43eee32",
   "metadata": {},
   "source": [
    "### Load data from disk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e53f9cd8-e1d0-4e91-a179-ada6f812ea50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/openai/lib/python3.10/site-packages/pandas/core/frame.py:706: DeprecationWarning: Passing a BlockManager to DataFrame is deprecated and will raise in a future version. Use public APIs instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (23905, 13)\n",
      "Memory Usage: 2.37+ MB\n"
     ]
    }
   ],
   "source": [
    "# docs = FileIO().load_parquet('../data/huberman_labs-minilm-256.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26ceb8-fabb-4007-85cc-967a763ab18f",
   "metadata": {},
   "source": [
    "### Create Schema and Index Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b01b14d0-5cf4-4269-a61a-4b0915fc1589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Huberman_minilm_256', 'Huberman_minilm_128', 'Huberman_minilm_512']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_name = 'Huberman_minilm_512'\n",
    "client.show_all_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4db436ad-ebee-4591-83cc-cbe1f3d8e07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection \"Huberman_minilm_256\" deleted\n"
     ]
    }
   ],
   "source": [
    "# client.delete_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d117190-7352-4834-a191-fcbcc51998f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection \"Huberman_minilm_512\" created\n"
     ]
    }
   ],
   "source": [
    "indexer.create_collection(collection_name, properties, description='Full index of 193 Huberman Labs episodes as of April 5, 2024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "457cde2c-7a42-4cf1-8643-6e6dbac8162c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11602/11602 [00:11<00:00, 976.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch job completed in 0.83 minutes.\n"
     ]
    }
   ],
   "source": [
    "batch = indexer.batch_index_data(docs, collection_name, properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb4b99-259e-428e-ae09-f90bb1cf5d37",
   "metadata": {},
   "source": [
    "## Small-to-Big Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eaee65d-0a33-42f3-b6f2-5c02945fa0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (26448, 12)\n",
      "Memory Usage: 2.42+ MB\n"
     ]
    }
   ],
   "source": [
    "data = FileIO().load_parquet('../impact-theory-new-ft-model-256.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d3555-d370-44b6-808b-9c91aba9bf7a",
   "metadata": {},
   "source": [
    "### Remove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91b9e46f-cb64-4791-ac97-8abbfe4e0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{k:v for k,v in d.items() if k != 'content_embedding'} for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54425b74-962b-4bea-bdb3-13fbc27abdfa",
   "metadata": {},
   "source": [
    "## Breakout Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7499b400-4875-4854-834e-dc93dafbd048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_into_episodes(data: List[dict]) -> List[list]:\n",
    "    '''\n",
    "    Separates entire Impact Theory corpus into individual \n",
    "    lists of discrete episodes.\n",
    "    '''\n",
    "    all_episodes = []\n",
    "    episode = []\n",
    "    cur_video = ''\n",
    "    count = 0\n",
    "    for d in data:\n",
    "        video_id = d['video_id']\n",
    "        if not cur_video:\n",
    "            cur_video = video_id\n",
    "        if cur_video == video_id:\n",
    "            episode.append(d)\n",
    "            count += 1\n",
    "        else:\n",
    "            all_episodes.append(episode)\n",
    "            count = 0\n",
    "            episode = []\n",
    "            episode.append(d)\n",
    "            cur_video = video_id\n",
    "    all_episodes.append(episode)\n",
    "    assert len(all_episodes) == 384\n",
    "    return all_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c614af7-7884-4541-86b7-c7d459a18679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def groupby_episode(data: List[dict], key_field: str='video_id') -> List[List[dict]]:\n",
    "    '''\n",
    "    Separates entire Impact Theory corpus into individual \n",
    "    lists of discrete episodes.\n",
    "    '''\n",
    "    episodes = []\n",
    "    for key, group in groupby(data, lambda x: x[key_field]):\n",
    "        episode = [chunk for chunk in group]\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be96980a-e3e9-4ce2-a999-0f9cdd61c581",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_episodes = groupby_episode(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbf6408-74ef-45f6-a41e-71a73e25ecfe",
   "metadata": {},
   "source": [
    "### Combine episode chunks into Parent Chunks one for each doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "77b07c0f-5ac2-4354-8007-c92c0e38eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_chunks(episode_list: List[list], window_size: int=2) -> List[dict]:\n",
    "    '''\n",
    "    Creates parent chunks from original chunk of text, for use with \n",
    "    small to big retrieval.  Window size sets number of chunks before\n",
    "    and after the original chunk.  For example a window_size of 2 will \n",
    "    return five joined chunks.  2 chunks before original, the original, \n",
    "    and 2 chunks after the original.  Chunks are kept in sequence by \n",
    "    using the doc_id field. \n",
    "    '''\n",
    "    parent_chunks = []\n",
    "    for episode in episode_list:\n",
    "        contents = [d['content'] for d in episode]\n",
    "        for i, d in enumerate(episode):\n",
    "            doc_id = d['doc_id']\n",
    "            start = max(0, i-window_size)\n",
    "            end = i+window_size+1\n",
    "            chunk = ' '.join(contents[start:end])\n",
    "            parent_chunks.append({doc_id:chunk})\n",
    "    return parent_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b80b321f-aa9b-457f-93fd-ba2dca2b29eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pchunks = create_parent_chunks(all_episodes, window_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a173eecb-8519-4abd-8750-6cac201fbae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_chunk_cache(parent_chunks: List[dict]) -> dict:\n",
    "    '''\n",
    "    Creates a simple in-memory cache for quick parent chunk lookup.\n",
    "    Used for small-to-big retrieval in a RAG system.\n",
    "    '''\n",
    "    content_cache = {}\n",
    "    for chunk in pchunks:\n",
    "        for k,v in chunk.items():\n",
    "            content_cache[k] = v\n",
    "    return content_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a5848323-a180-4842-a175-743d6d064c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = create_parent_chunk_cache(pchunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "da970c57-1e4a-4403-b7fc-edd7bb21b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "alltext = ' '.join(list(cache.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "01bc60d7-732f-4d68-bbc1-374e8146e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d1658bec-be47-41a8-98b7-359619f3b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model('gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fae36f03-c194-47aa-8cdb-1efee54a9307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40777582"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoding.encode(alltext))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe07f1ec-f469-463b-b323-ec883c10c4e6",
   "metadata": {},
   "source": [
    "# ------------------- BREAK ---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ade2eb-f853-4fe6-a273-a21a29bf01fd",
   "metadata": {},
   "source": [
    "# Small to Big Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "69a8bc3b-5760-4688-afbc-96cbf741487f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Size of original data: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26448</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Size of original data: \u001b[1;36m26448\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Size of cached content: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26448</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Size of cached content: \u001b[1;36m26448\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Size of original data: {len(data)}')\n",
    "print(f'Size of cached content: {len(content_cache)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd34d184-fd63-4aa0-851b-9e01e751d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name =  'Fine_tuned_on_300'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd773b-742d-4f04-8d10-90f3e92ff271",
   "metadata": {},
   "source": [
    "### Hybrid Search call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "592260e6-6583-4d9d-9e88-d6b91016b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'does this show discuss the use of generative ai'\n",
    "response = client.hybrid_search('does this show discuss the use of generative ai', \n",
    "                                collection_name = 'Huberman_minilm_128',\n",
    "                                query_properties=['content', 'short_description', 'guest'], alpha=0.45)\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05f9bd98-6aae-4af0-9144-819ca47b5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.reranker import ReRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d38c101e-7050-4e24-ac56-61ffa0588492",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = ReRanker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14aae396-6762-4c2c-aef9-0d0592ff8016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'guest': 'Dr. Paul Conti',\n",
       "  'content': \"And by now in this episode, I'm sure people are well on board the understanding that the generative drive is not just about going out and doing things. It's about doing things in service to and in a way that supports learning, knowing, creating, not just of others and in the world, but inside. Yes, yes. I love the map imagery because you can almost see the map changing, right? As a person, I imagine the person is busying away in the cupboards, right?\",\n",
       "  'title': 'Dr. Paul Conti: How to Build and Maintain Healthy Relationships | Huberman Lab Guest Series',\n",
       "  'video_id': 'eMqWH3LYiII',\n",
       "  'score': 0.44999998807907104,\n",
       "  'cross_score': 0.27966332},\n",
       " {'guest': 'Marc Andreessen',\n",
       "  'content': \"In doing so, Mark provides a stark counter-argument for those that argue that AI is going to diminish human experience. So if you're hearing about and or concerned about the ways that AI is likely to destroy us, today you are going to hear about the many different ways that AI technologies now in development are likely to enhance our human experience at every level. What you'll soon find is that while today's discussion does center around technology and technology development, it is really a discussion about human beings and human psychology.\",\n",
       "  'title': 'Marc Andreessen: How Risk Taking, Innovation & Artificial Intelligence Transform Human Experience',\n",
       "  'video_id': 'yixIc1Ai6jM',\n",
       "  'score': 0.447770357131958,\n",
       "  'cross_score': 0.008838829},\n",
       " {'guest': 'Dr. Paul Conti',\n",
       "  'content': \"I think that there were years in graduate school where I wanted to publish a bunch of papers and then quickly realized through the not so gentle persuasion of my mentors that like, let's just do the best possible work we can do. And there's so much more richness and experience and things to be gained from that. So I'm familiar with generative drive as I understand it, but maybe if you would, if you could flesh out a bit of what generative drive is and does it arrive in parallel with or before we are able to access peace, contentment, and delight?\",\n",
       "  'title': 'Dr. Paul Conti: How to Understand & Assess Your Mental Health | Huberman Lab Guest Series',\n",
       "  'video_id': 'tLRCS48Ens4',\n",
       "  'score': 0.4278365671634674,\n",
       "  'cross_score': 0.0041087563},\n",
       " {'guest': 'Dr. Paul Conti',\n",
       "  'content': \"But it also seems to be the feature that if not kept in check, if we can't regulate our own anxiety, I'm realizing there's no way that we're going to be in a position to ask for what we need and what we want, to hear what's needed of us and what others want. In other words, anxiety seems like a major barrier to the generative drive. I think the place to start is if you show me a person who has no anxiety, I'll show you a mannequin. We all have anxiety in us. Remember, it's just a word.\",\n",
       "  'title': 'Dr. Paul Conti: How to Build and Maintain Healthy Relationships | Huberman Lab Guest Series',\n",
       "  'video_id': 'eMqWH3LYiII',\n",
       "  'score': 0.3601960837841034,\n",
       "  'cross_score': 0.004055965},\n",
       " {'guest': 'Dr. Paul Conti',\n",
       "  'content': \"And we also explored a little bit of how it can be bad for a relationship if it exceeds the generative drive. What about the aggressive proactiveness drive? How does that show up in romantic pairings? You know, if one person has a high proactiveness, aka aggressive drive, and the other person does not's a two and the person who's an eight on the sex drive scale, right? You'd say, okay, the person who's a two who's trying to raise that, right? Has a strong generative drive. What does that mean?\",\n",
       "  'title': 'Dr. Paul Conti: How to Build and Maintain Healthy Relationships | Huberman Lab Guest Series',\n",
       "  'video_id': 'eMqWH3LYiII',\n",
       "  'score': 0.550000011920929,\n",
       "  'cross_score': 0.002465098},\n",
       " {'guest': 'Marc Andreessen',\n",
       "  'content': \"You have to use AI, you have to use this other architecture, and you have to basically teach them how to recognize objects and images at high speeds, basically the same way the human brain does. And so those are so-called neural networks running inside. So essentially, let the machine operate based on priors. We almost clipped a boulder going up this particular drive. And so therefore, this shape that previously the machine didn't recognize as a boulder, it now introduces to its catalog of boulders.\",\n",
       "  'title': 'Marc Andreessen: How Risk Taking, Innovation & Artificial Intelligence Transform Human Experience',\n",
       "  'video_id': 'yixIc1Ai6jM',\n",
       "  'score': 0.3174770772457123,\n",
       "  'cross_score': 0.0018309889},\n",
       " {'guest': 'Marc Andreessen',\n",
       "  'content': \"Yeah, conceivably. Yeah, not yet, but yeah, someday. So going back to AI, most people who hear about AI are afraid of AI. Well, I think most people who aren't informed. This goes back to our elites versus masses thing. Oh, interesting. Well, I heard you say that, and this is from a really wonderful tweet thread that we will link in the show note captions that you put out not long ago and that I've read now several times and that everyone really should take the time to read.\",\n",
       "  'title': 'Marc Andreessen: How Risk Taking, Innovation & Artificial Intelligence Transform Human Experience',\n",
       "  'video_id': 'yixIc1Ai6jM',\n",
       "  'score': 0.5531707406044006,\n",
       "  'cross_score': 0.0015332929},\n",
       " {'guest': 'Marc Andreessen',\n",
       "  'title': 'Marc Andreessen: How Risk Taking, Innovation & Artificial Intelligence Transform Human Experience',\n",
       "  'content': \"It probably takes about 20 minutes to read it carefully and to think about each piece and it's highly recommend it. But you said, and I'm quoting here, let's address the fifth, the one thing I actually agree with, which is AI will make it easier for bad people to do bad things. Yeah. Yeah. Well, so, so yes. So, so, so, so, so, so, yeah. So, so first of all, there is a general freak out happening around AI.\",\n",
       "  'video_id': 'yixIc1Ai6jM',\n",
       "  'score': 0.29385602474212646,\n",
       "  'cross_score': 9.2421484e-05},\n",
       " {'guest': 'Marc Andreessen',\n",
       "  'content': \"So look, there's a reason actually why Google is so good and Apple is not right now with that kind of thing. And it actually goes to actually the, it's actually an ideological thing of all things. Apple does not permit pooling of data for any purpose, including training AI, whereas Google does. And Apple's just like stake their brand on privacy and among that is sort of a pledge that they don't like pool your data. And so all of Apple's AI is like AI that has to happen like locally on your phone, whereas Google's AI can happen in the cloud, right?\",\n",
       "  'title': 'Marc Andreessen: How Risk Taking, Innovation & Artificial Intelligence Transform Human Experience',\n",
       "  'video_id': 'yixIc1Ai6jM',\n",
       "  'score': 0.32054492831230164,\n",
       "  'cross_score': 3.7140635e-05},\n",
       " {'guest': 'Marc Andreessen',\n",
       "  'title': 'Marc Andreessen: How Risk Taking, Innovation & Artificial Intelligence Transform Human Experience',\n",
       "  'content': \"If you're me, you always click, right? Because you're like, what's behind the scrim? And then, or this is a, I don't always look at the, this image is gruesome type thing. Sometimes I just pass on that. But if it's something that seems debatable, of course you look. Well, and you should have an AI assistant with you when you're on the internet, and you should be able to tell that AI assistant what you want, right? So yes, I want the full experience to show me everything.\",\n",
       "  'video_id': 'yixIc1Ai6jM',\n",
       "  'score': 0.34037724137306213,\n",
       "  'cross_score': 2.1687138e-05}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\xfdR\\xbe8@\\x93\\\\\\xd3\\x15\\xd2\\xc2\\x1a\\xc2\\xec\\x07\\xc97\\x05 _j\\x82\\xb6U0\\xb2\\x012\\xb4\\xb7]', b\"\\xef\\x1af\\xf4,\\xe4\\xce{Q\\x1b\\n\\xcb\\xed'QuV\\xce\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\"]\n",
      "Bad pipe message: %s [b'\\x01\\x02']\n",
      "Bad pipe message: %s [b'\\t\\x88\\xac\\xa9\\xd5\\xaf@\\xf2\\xaaId\\xb1\\xfd\\x0e\\x9d%WE &\\xc2;\\xc3{\\xba?ZS\\xf4g7\\xf7\\xfe\\xb9\\xae\\x88(J\\xdb\\x8aR|\\xbe)\\x84\\xc5\\x17\\xdd\\xe7\\xaeR\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f']\n",
      "Bad pipe message: %s [b'\\x15\\xf4b\\xb0\\x15\\xb01\\x1d\\x12\\x16\\xdc\\xf3\\xac\\x1a\\xc8K\\xf32\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0', b\"\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\"]\n",
      "Bad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'q\\xc7NMr\\xe4N\\x17\\xc1\\xd2\\xc1]j\\xd0\\x92\\xec\\xe5\\xbd\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00', b'']\n",
      "Bad pipe message: %s [b'\\xe8\\x88d\\xa9cn\\xac4\\x14\\xcc', b'q\\x08\\xe5\\xf5a_\\x00\\x00\\xa2']\n",
      "Bad pipe message: %s [b'\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02']\n",
      "Bad pipe message: %s [b'o)\\xe6dh\\x83\\\\\\x9f\\xcc\\xb5\\x8f\\xdf\\t\\xc1\\x0b\\xb2\\x05\\x07\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00']\n",
      "Bad pipe message: %s [b'\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00']\n",
      "Bad pipe message: %s [b'\\x17\\x00\\x03\\xc0\\x10']\n",
      "Bad pipe message: %s [b'\\x8c\\xd5s\\xc4l\\xc8\\x050c\\xb0\\x80\\xd3^+g\\x97\\x8d\\xc0']\n",
      "Bad pipe message: %s [b\"\\x94r\\x0eV\\xb2\\x04\\xf4\\x02\\xf8\\xe5\\t1\\xbdM,\\xec~\\x13\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0\"]\n",
      "Bad pipe message: %s [b')\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00']\n",
      "Bad pipe message: %s [b'\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12']\n"
     ]
    }
   ],
   "source": [
    "reranker.rerank(response, query, apply_sigmoid=True, top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb136c-ed06-475e-9af0-dd16c4f4a27c",
   "metadata": {},
   "source": [
    "### View larger context from response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "25d6608f-81cb-48cc-8af2-fccb8a1c6970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So your chief AI officer is scanning the horizon, understanding it, and then advising members of your team. So every part of your team, right? There's going to be AI supporting sales, and marketing, and engineering, and HR. We're all going to have, in the near term, an AI co-pilot, right? This is an AI that helps you do your job better, because we are so limited as carbon life forms. But ultimately is going to be able to operate and do a number of the things repetitively, because we do a lot of repetitive tasks, and AIs are much better at that. I think if you've got, we've got, say, a 30 person company, every single person needs to be trained in AI, and using these chatbot auto GPT tools, and absolutely augment themselves 10, 20, 100x. I have said to my company, okay, everybody here needs to figure out in your department, what are the tools that exist in AI? And how can you immediately implement them? But even that's pretty vague. Like I'm just sort of dumping it on them. Where do people start? What is the thing you actually do? Easy and super specific. If you have an email newsletter that goes out, use chat GPT, just say, how would I increase the engagement rate with this email? We did that. We got a 25% increase. Can you feed it the email? You feed it the email. Yeah. And just say, how do I make this better? Yeah. Come up with a better headline. Or put in social sharing links throughout it. Or say, listen, I'm in HR, go to chat GPT, open it up right now. Hopefully you have the GPT-4 version of it. And say, I'm in HR. How should I be using generative AI in my business? It'll feed you. Give me five examples or 10 examples. Pick the one that sounds good. Give me step-by-step instructions on how to use this. It's recursive in that fashion. And so you're going to use AI to help you learn what you want to know. It comes back a lot to mindset, Tom. And you need the mindset of a kid here. Curiosity, absolute play. It's like one of the things I'm going to be doing in my team, my PhD Ventures that runs Abundance360 and a few others, we're setting aside three days. And no homework coming out of these three days. We're going to go in with a series of objectives. And we're going to actually crank for three days and generate all the content, all the plans. And you can. But it takes time for all of us to switch from our old habits of how we do things to new ways. So the first time it's going to take 150% of your time. The next time we'll take 50%, then 25%. You talk about mindset. The thing I see, and I'm sure you guys have encountered this, is a lot of people, they just have so much anxiety about this is going to replace me. I think about that a lot. So Lisa and I have put our fortune back at risk to build this company. And you've said this a lot. I've said this a lot. Skate to where the puck is going to be, not where the puck is.\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_cache['zm0QVutAkYg_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e10b2-8ba7-409d-8857-ff049876ce32",
   "metadata": {},
   "source": [
    "### Extract top-n results from response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f10e7070-6154-47c4-8f7f-b7fdaa0dad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 3\n",
    "\n",
    "def get_top_n(response: List[dict], top_n: int=3):\n",
    "    top_docs = [d['doc_id'] for d in response[:top_n]]\n",
    "    cache_responses = [content_cache[doc_id] for doc_id in top_docs]\n",
    "    return cache_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4c0aac4d-e5b4-4770-b460-38cdc733d39a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"So your chief AI officer is scanning the horizon, understanding it, and then advising members of your team. So every part of your team, right? There's going to be AI supporting sales, and marketing, and engineering, and HR. We're all going to have, in the near term, an AI co-pilot, right? This is an AI that helps you do your job better, because we are so limited as carbon life forms. But ultimately is going to be able to operate and do a number of the things repetitively, because we do a lot of repetitive tasks, and AIs are much better at that. I think if you've got, we've got, say, a 30 person company, every single person needs to be trained in AI, and using these chatbot auto GPT tools, and absolutely augment themselves 10, 20, 100x. I have said to my company, okay, everybody here needs to figure out in your department, what are the tools that exist in AI? And how can you immediately implement them? But even that's pretty vague. Like I'm just sort of dumping it on them. Where do people start? What is the thing you actually do? Easy and super specific. If you have an email newsletter that goes out, use chat GPT, just say, how would I increase the engagement rate with this email? We did that. We got a 25% increase. Can you feed it the email? You feed it the email. Yeah. And just say, how do I make this better? Yeah. Come up with a better headline. Or put in social sharing links throughout it. Or say, listen, I'm in HR, go to chat GPT, open it up right now. Hopefully you have the GPT-4 version of it. And say, I'm in HR. How should I be using generative AI in my business? It'll feed you. Give me five examples or 10 examples. Pick the one that sounds good. Give me step-by-step instructions on how to use this. It's recursive in that fashion. And so you're going to use AI to help you learn what you want to know. It comes back a lot to mindset, Tom. And you need the mindset of a kid here. Curiosity, absolute play. It's like one of the things I'm going to be doing in my team, my PhD Ventures that runs Abundance360 and a few others, we're setting aside three days. And no homework coming out of these three days. We're going to go in with a series of objectives. And we're going to actually crank for three days and generate all the content, all the plans. And you can. But it takes time for all of us to switch from our old habits of how we do things to new ways. So the first time it's going to take 150% of your time. The next time we'll take 50%, then 25%. You talk about mindset. The thing I see, and I'm sure you guys have encountered this, is a lot of people, they just have so much anxiety about this is going to replace me. I think about that a lot. So Lisa and I have put our fortune back at risk to build this company. And you've said this a lot. I've said this a lot. Skate to where the puck is going to be, not where the puck is.\",\n",
       " \"We showed this prototype of something called BuilderBot, which is basically a helper for people creating worlds in Horizon. And the idea is instead of, and we have this whole scripting language where you can be in VR and you can kind of drag things around and lay out the world the way you want, which is pretty wild and fun. It's like the first creation tool where you're in the thing that you're creating as you're building it, right? So you're not writing a script and compiling it and seeing what it looks like or drawing something on a screen in Photoshop. You're like in it and building it. But now with AI, we also have the tools so you can just say, okay, put a tree over there. Actually, I want a tree. Can you maybe make that tree have more branches? All right. Maybe it's false. So make the tree, maybe the branches should be, the leaves should be turning red and yellow instead of green. It's like, okay, put a waterfall there, whatever, however you want to design this thing. Being able to script it or kind of put it together with your hands is one thing, but also being able to use AI to just describe the world that you want to create and have an AI help build it. Do you have to load the assets or can the AI actually construct the trees, the clouds, the whatever? I mean, I think over time it'll get even more generative, but I mean, there's a whole roadmap here as well. Right now we're mostly focused on, okay, it knows a certain number of things and we want you to be able to express those. But I mean, in one of the demos that I did, it was like, all right, here, put some clouds in the sky. It's like actually make them cumulonimbus clouds. And it like knew, you know, we were able to make it so that it kind of had a sense of what that was and it made clouds that were accurate to that. So over time, I mean, obviously it's going to need to know what the concepts are, but being able to be generative, even for things that it doesn't have texture for or something like that, I do think is where we want to go with that. But there's a long roadmap on that too. How do you think about ownership and all of this? So even as you were describing that scenario where the AI, if the AI isn't procedurally generating it, can people create items that they contribute to that, but they put a price tag on it? Like, how do you think about that? That seems to be in web three, people that are hardcore web three, they love themselves some decentralization and they definitely love ownership. Where do you come down on those two really big ideas? Yeah. I mean, one of the things that we're working on for Horizon is basically the ability to just import anything that you make outside and have an asset store around this that people can exchange or buy things.\",\n",
       " \"So for people that don't understand how the art is created, it looks at a field of noise. Here are all the possible things that these could be in any of these pixels. And from that field of possibilities, it pulls forth the most likely placement of pixels and colors based on what you type. That's insane. So that level of pattern recognition, as evidenced by the art that it can generate, is truly mind blowing. So this guy's saying, OK, hey, at least ideas will be the last bastion and you'll never be able to get rid of me, the artist, because I'm the one with taste. I'm the one with good ideas. Not realizing, no, no, no, what AI is, is a pattern recognition machine. It will recognize the greatest ideas that have ever been had, what they have in common, and will be able to predict the next great idea along that thing. It doesn't even have to just regurgitate what it's already seen. It can like figure out what that sequence is and what that next part of the sequence could be. And on top of that, it's doing that with humans. So AI will get, AI is already extraordinarily good. This is why people think their phone's recording them when it serves an ad. Oftentimes, Target, using their AI, knows that you're pregnant before you do if you're a woman, because they know what to pick up on. So AI is going to get extremely good at understanding us at an individual level, serving us up exactly what we want right in that moment. And that gets dystopian really fast. Really fast. I mean, again, when you combine it with the social credit score, as you've seen in kind of China and other things, you gamify life and you have a system of complete social control, a panopticon, as it were. The pattern recognition was the missing bit whereby you had a level of pattern recognition. So for taste, what do you have? TikTok, Shine, hundred billion dollar companies based on old school algorithms before we even got to generative AI, which as you said, it can take images out of noise. Stable diffusion, you know, the model that we collaborated on now that we lead, we took a hundred thousand gigabytes of images and the output was a two gigabyte file that acts as a filter. Words go in, images come out. Because why is that discrepancy in size meaningful? Fifty thousand to one compression is not WinZip. If you remember Silicon Valley on HBO, it's way beyond that they managed there in terms of compression. It's unheard of compression. Is it compression or is it something completely different? It's intelligence. It's learning the principles. How much information do you see? And then you learn the principles and then spot the tiger in the bush. You learn what's next. Literally GPT and these language models. They predict the next word. That's all they do. They pay attention. They predict the next word. And that was the missing part to intelligence that now is there. We've had the first studies now come out that show that the language models score higher in creativity than people. Woof. And again, think about TikTok. Think about Shine.\"]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18cbe2-fae6-4d3c-8e03-0ae740f62acb",
   "metadata": {},
   "source": [
    "### Compare with original response content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "242f3e70-7818-434e-963c-316386f39190",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_content = [d['content'] for d in response[:top_n]]\n",
    "response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4e191918-03c7-4dee-9b20-8593d9a920fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7607226a-81e9-4eec-bc1b-79c44be8770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tiktoken.encoding_for_model('gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "67244738-2d72-40ef-9e45-05c4a5a057c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/elastic/notebooks/datasets/acled_reports/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7798ba20-1474-43f6-8a47-64c921cf1568",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = sorted([os.path.join(data_path, file) for file in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, file))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "71054e5c-38dc-4a66-84d5-072bea91c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = []\n",
    "\n",
    "for path in paths:\n",
    "    with open(path) as f:\n",
    "        string = f.read()\n",
    "        strings.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8a1bf6a0-3865-458c-a40a-8358ceb7c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(list(map(len, encoder.encode_batch(strings))), columns=['lens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8e50c3d7-b380-40da-9d81-7879b777e2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lens    21279\n",
       "dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef0efc-f151-4000-9502-91564b212560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vsa",
   "language": "python",
   "name": "vsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
