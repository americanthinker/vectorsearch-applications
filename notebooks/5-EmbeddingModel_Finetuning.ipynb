{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40a21504-0b5f-4e74-95c8-d7b2f749f66b",
   "metadata": {},
   "source": [
    "# Fine-Tuning an Embedding Model\n",
    "***\n",
    "#### This exercise won't cost you anything except some time...\n",
    "\n",
    "#### It is recommended to run this specific section either locally on your machine or on Google Colab. As a proxy, it will take less than 10 minutes to run in a Macbook Air M1.\n",
    "\n",
    "Aside from switching out your emebdding model to improve your retrieval results, you could also try fine tuning your embedding model (or better yet, switch out your model and then fine tune the new one...ðŸ‘Š).  For the longest time, the problem with fine-tuning sentence embedding models was the lack of access to high quality training data.  Generative LLMs can save you days/weeks of time, depending on how large of a dataset you want to create, by automating the process of generating high quality query/context pairs.  In this section we'll go over the step-by-step process of fine-tuning our `all-MiniLM-L6-v2` embedding model from a pre-generated training dataset consisting of only 300 question-context pairs, and then comparing it's retrieval results to our baseline retrieval scores.  I highly encourage trying this method out, I saw a 10+ point jump in `vector_hit_rate` after fine-tuning the baseline model.\n",
    "\n",
    "### Fine-tune Walkthrough\n",
    "\n",
    "1. Get baseline retrieval scores (vector Hit Rate, MRR, and total misses) using out-of-the-box baseline model.  You won't know objectively if fine-tuning had any effect if you don't measure the baseline results first.  I know this goes without saying it, but practitioners sometimes want to jump straight into model improvement without first considering their starting point.\n",
    "2. Collect a training and validation dataset.  This step has already been completed for you, courtesy of `gpt-3.5-turbo`.  LlamaIndex has a great out-of-the-box solution for generating query/context embedding pairs, but it isn't exactly plug and play, so I had to rewrite the function to achieve comptability for our course.  The training dataset consists of queries generated by the LLM that can be answered from the associated context (text chunk).  These pairs were generated using a prompt specifically written for the Impact Theory corpus so the training and validation data (for the most part) are high quality and contextually relevant. \n",
    "3. Instantiate a `SentenceTransformersFinetuneEngine` Class written by LlamaIndex which does a great job of abstracting away most of the details invovled in fine-tuning a Sentence Transformer model.\n",
    "4. Fit the model and set a path where the new model will reside.  I creaed a `models/` directory in the course repo, and included the directory in the `.gitignore` file so that models aren't being pushed with every commit.\n",
    "5. Create a new dataset (as you learned in Notebook 1) but this time create the embeddings using the new fine-tuned model.\n",
    "6. Create a new index on Weaviate using the new dataset you just created.\n",
    "7. Run the `retrieval_evaluation` function again, but this time instantiate your Weaviate client with the new fine-tuned model, but hold all other parameters constant (i.e. don't change any other parameter from the baseline run).\n",
    "8. Compare the fine-tuned retrieval results to the baseline results ðŸ¥³\n",
    "\n",
    "### Import Training + Valid datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a78b81c-2019-4316-8d59-574c91ac9c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = './data/training_data_300.json'\n",
    "valid_path = './data/validation_data_100.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e8b792-8856-4ca7-a2d3-f6d196d926c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = EmbeddingQAFinetuneDataset.from_json(training_path)\n",
    "valid_set = EmbeddingQAFinetuneDataset.from_json(valid_path)\n",
    "num_training_examples = len(training_set.queries)\n",
    "num_valid_examples = len(valid_set.queries)\n",
    "print(f'# Training Samples: {num_training_examples}\\n# Validation Samples: {num_valid_examples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ef4046-6622-43fe-a89d-d94f14fa9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#always a good idea to name your fine-tuned so that you can easily identify it,\n",
    "#especially if you plan on doing multiple training runs with different params\n",
    "#also probably a good idea to include the # of training samples you are using in the name\n",
    "\n",
    "model_id = client.model_name_or_path\n",
    "model_ext = model_id.split('/')[1]\n",
    "models_dir = './models'\n",
    "if not os.path.exists('./models'):\n",
    "    os.makedirs('./models') \n",
    "else:\n",
    "    print(f'{models_dir} already exists')\n",
    "ft_model_name = f'finetuned-{model_ext}-{num_training_examples}'\n",
    "model_outpath = os.path.join(models_dir, ft_model_name)\n",
    "\n",
    "print(f'Model ID: {model_id}')\n",
    "print(f'Model Outpath: {model_outpath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f08636-49d5-4248-b873-20ee6ea118da",
   "metadata": {},
   "source": [
    "### Instantiate your Fine-tune engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d64e12-0279-42e7-a516-fa33f6dea68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import SentenceTransformersFinetuneEngine\n",
    "\n",
    "finetune_engine = SentenceTransformersFinetuneEngine(\n",
    "    training_set,\n",
    "    batch_size=32,\n",
    "    model_id=model_id,\n",
    "    model_output_path=model_outpath,\n",
    "    val_dataset=valid_set,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a6368d-55e5-4f91-a21c-81b3950b6715",
   "metadata": {},
   "source": [
    "### Fit the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3d8c5e5-3dcc-42b9-a81a-f60caf9f2b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc83673-ac88-47cf-ac12-d5120345a125",
   "metadata": {},
   "source": [
    "The `finetune` method will automatically generate the model directory using the `model_output_path` that you define.  Inside the directory will be a copy of the model itself (`pytorch_model.bin`) along with all the other files it needs.  Also in that folder, assuming you provide a `val_dataset` will be an evaluation report in the `eval` directory.  The evaluation report contains several IR metrics that may or may not be useful to you, but it does allow you to compare score improvements with each training epoch.  The new fine-tuned model is loaded through the `SentenceTransformer` class just like any other HuggingFace repo model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe6057-baba-4eb8-ad45-569210f34729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vsa",
   "language": "python",
   "name": "vsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
